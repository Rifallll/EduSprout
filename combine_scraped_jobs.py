import json
from datetime import datetime

# List of JSON files to combine.
# These should be the intermediate files generated by your individual scraping/exporting scripts.
INPUT_FILES = [
    "src/data/jobs_from_db.json",        # Data exported from your jobs.db (e.g., Lokerbandung, GetRedy, Jooble)
    "src/data/jobs_from_jobstreet.json"  # Data from JobStreet (assuming you have a script for this)
    # Add more paths here for other sources if needed, e.g., "src/data/jobs_from_linkedin.json"
]
OUTPUT_PATH = "src/data/scrapedJobsFromDB.json" # The final combined file for your React app

combined_jobs_map = {} # Use a dictionary to easily handle unique IDs and prevent duplicates

for file_path in INPUT_FILES:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            for job in data:
                if "id" in job:
                    # Add or overwrite if ID exists. The last file in INPUT_FILES list wins for duplicates.
                    combined_jobs_map[job["id"]] = job
                else:
                    print(f"Warning: Job in {file_path} has no 'id' field and will be skipped.")
    except FileNotFoundError:
        print(f"Warning: File not found at {file_path}. Skipping this source.")
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from {file_path}. Skipping this source.")

# Convert the dictionary of unique jobs back to a list
final_jobs_list = list(combined_jobs_map.values())

# Sort the combined list by date_posted in descending order (newest first)
# Assuming 'date_posted' is in a sortable format (e.g., YYYY-MM-DD)
final_jobs_list.sort(key=lambda x: x.get("date_posted", ""), reverse=True)

with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
    json.dump(final_jobs_list, f, ensure_ascii=False, indent=2)

print(f"âœ… {len(final_jobs_list)} combined jobs exported to {OUTPUT_PATH} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")